{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 98308 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open('shakesonnets.txt', 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\r\n",
      "\r\n",
      "From fairest creatures we desire increase,\r\n",
      "That thereby beauty's rose might never die,\r\n",
      "But as the riper should by time decease,\r\n",
      "His tender heir might bear his memory:\r\n",
      "But thou contracted to thine own bright eyes,\r\n",
      "Feed'st thy light's flame w\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  \"'\" :   4,\n",
      "  '(' :   5,\n",
      "  ')' :   6,\n",
      "  ',' :   7,\n",
      "  '-' :   8,\n",
      "  '.' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I\\r\\n\\r\\nFrom fai' ---- characters mapped to int ---- > [21  1  0  1  0 18 54 51 49  2 42 37 45]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "\r\n",
      "\n",
      "\n",
      "\r\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I\\r\\n\\r\\nFrom fairest creatures we desire increase,\\r\\nThat thereby beauty's rose might never die,\\r\\nBut as \"\n",
      "'the riper should by time decease,\\r\\nHis tender heir might bear his memory:\\r\\nBut thou contracted to thi'\n",
      "\"ne own bright eyes,\\r\\nFeed'st thy light's flame with self-substantial fuel,\\r\\nMaking a famine where abu\"\n",
      "\"ndance lies,\\r\\nThy self thy foe, to thy sweet self too cruel:\\r\\nThou that art now the world's fresh orn\"\n",
      "'ament,\\r\\nAnd only herald to the gaudy spring,\\r\\nWithin thine own bud buriest thy content,\\r\\nAnd tender c'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"I\\r\\n\\r\\nFrom fairest creatures we desire increase,\\r\\nThat thereby beauty's rose might never die,\\r\\nBut as\"\n",
      "Target data: \"\\r\\n\\r\\nFrom fairest creatures we desire increase,\\r\\nThat thereby beauty's rose might never die,\\r\\nBut as \"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 21 ('I')\n",
      "  expected output: 1 ('\\r')\n",
      "Step    1\n",
      "  input: 1 ('\\r')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    2\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 1 ('\\r')\n",
      "Step    3\n",
      "  input: 1 ('\\r')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    4\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 18 ('F')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    rnn(rnn_units,\n",
    "        return_sequences=True,\n",
    "        recurrent_initializer='glorot_uniform',\n",
    "        stateful=True),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 63) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (32, None, 256)           16128     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_15 (CuDNNGRU)      (32, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (32, None, 63)            64575     \n",
      "=================================================================\n",
      "Total params: 4,019,007\n",
      "Trainable params: 4,019,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38,  4, 28,  7, 42, 43, 42, 41, 20, 14, 48, 57, 16, 59, 28, 38, 27,\n",
       "        1,  2, 28, 23,  6,  2, 32, 21,  4, 56, 54, 17, 34, 60, 60, 41, 62,\n",
       "       15, 45, 50, 32, 28, 42, 54, 15, 48, 29, 22, 57, 36, 46, 21, 30, 46,\n",
       "       29, 23, 16, 50, 38, 29, 16, 15, 37, 58, 62, 41,  4, 25,  5, 36,  6,\n",
       "       20, 62, 34, 32, 33, 50, 59, 57, 18, 37, 60, 43, 35, 27,  6, 53, 58,\n",
       "       15, 54,  0, 50, 14, 46, 17, 49, 48, 60, 38, 38,  8, 41, 60])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"rit heaven's graces,\\r\\nAnd husband nature's riches from expense;\\r\\nThey are the lords and owners of th\"\n",
      "\n",
      "Next Char Predictions: \n",
      " \"b'P,fgfeHBluDwPbO\\r PK) UI'trEWxxezCinUPfrClRJuYjISjRKDnbRDCavze'M(Y)HzWUVnwuFaxgXO)qvCr\\nnBjEmlxbb-ex\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 100, 63)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.143518\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.backend.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 4.0400\n",
      "Epoch 2/25\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 2.6783\n",
      "Epoch 3/25\n",
      "30/30 [==============================] - 2s 74ms/step - loss: 2.3106\n",
      "Epoch 4/25\n",
      "30/30 [==============================] - 2s 71ms/step - loss: 2.1707\n",
      "Epoch 5/25\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 2.0771\n",
      "Epoch 6/25\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.9981\n",
      "Epoch 7/25\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.9280\n",
      "Epoch 8/25\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.8647\n",
      "Epoch 9/25\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.7985\n",
      "Epoch 10/25\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.7419\n",
      "Epoch 11/25\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.6883\n",
      "Epoch 12/25\n",
      "30/30 [==============================] - 2s 65ms/step - loss: 1.6399\n",
      "Epoch 13/25\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 1.5890\n",
      "Epoch 14/25\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.5436\n",
      "Epoch 15/25\n",
      "30/30 [==============================] - 2s 70ms/step - loss: 1.4974\n",
      "Epoch 16/25\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.4501\n",
      "Epoch 17/25\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.4038\n",
      "Epoch 18/25\n",
      "30/30 [==============================] - 2s 82ms/step - loss: 1.3564\n",
      "Epoch 19/25\n",
      "30/30 [==============================] - 2s 69ms/step - loss: 1.3094\n",
      "Epoch 20/25\n",
      "30/30 [==============================] - 2s 75ms/step - loss: 1.2554\n",
      "Epoch 21/25\n",
      "30/30 [==============================] - 2s 73ms/step - loss: 1.2024\n",
      "Epoch 22/25\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.1442\n",
      "Epoch 23/25\n",
      "30/30 [==============================] - 2s 68ms/step - loss: 1.0807\n",
      "Epoch 24/25\n",
      "30/30 [==============================] - 2s 66ms/step - loss: 1.0211\n",
      "Epoch 25/25\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 0.9542\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_25'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (1, None, 256)            16128     \n",
      "_________________________________________________________________\n",
      "cu_dnngru_16 (CuDNNGRU)      (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (1, None, 63)             64575     \n",
      "=================================================================\n",
      "Total params: 4,019,007\n",
      "Trainable params: 4,019,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "God;\r\n",
      "My speek not so foll worse will perverce.\r\n",
      "  This flander, spirit a sower doth:\r\n",
      "But thou tonce thou be giving hell:\r\n",
      "  Yet teal\r\n",
      "  Against my silveren of no face were know,\r\n",
      "Tund trif, that more grace the salv as end of well they should not be such strave\r\n",
      "Which, loved out of sectrased socrew. \r\n",
      "  Or, if the still my count. \r\n",
      "In the old to my beauty's sace work wrong,\r\n",
      "  Painting their virwer fur doth be;\r\n",
      "  If my love, in every vowour sight?\r\n",
      "O, he delightless self o'er what I for we,\r\n",
      "  The chilsing hose; the fair he say,\r\n",
      "And made chiser as deep recernantw'll; and my love'.\r\n",
      "\r\n",
      "XLII\r\n",
      "\r\n",
      "Why gower evangely what a score;\r\n",
      "Whereto the beauty of my life-depart:\r\n",
      "I himple arm those brass, nor in love that which flown,\r\n",
      "But her pporat that when thou art.\r\n",
      "Gle agening thy beauties were not blessed.\r\n",
      "Have beauty's pleckons to tee nure.\r\n",
      "\r\n",
      "LVII\r\n",
      "\r\n",
      "What cankern of lawe, though they would have I sweet.\r\n",
      "\r\n",
      "CXXVV\r\n",
      "\r\n",
      "Then can I dream when thy fear, wrom who kind ever-pow that I love a women's s\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"God\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
